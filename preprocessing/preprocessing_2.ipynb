{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Data Preparation - Steam Games Dataset\n",
        "\n",
        "## Objective\n",
        "Prepare the preprocessed dataset for machine learning:\n",
        "- Handle null values\n",
        "- Encode categorical variables (developers, publishers, genres, categories, languages)\n",
        "- Scale/normalize numerical features\n",
        "- Split into train/test sets\n",
        "\n",
        "## Dataset\n",
        "- **Input:** `../archive1/games_march2025_ml_ready.csv` (from preprocessing notebook)\n",
        "- **Output:** ML-ready features and train/test splits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>.container { width:90% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session created successfully!\n",
            "Spark version: 4.1.0\n"
          ]
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, isnan, isnull, regexp_replace, split, size, array, lit\n",
        "from pyspark.sql.types import IntegerType, DoubleType, StringType\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, MinMaxScaler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.stat import Correlation\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"MLDataPreparation\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce output noise\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"Spark session created successfully!\")\n",
        "print(f\"Spark version: {spark.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Preprocessed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded:\n",
            "Total number of games: 11,889\n",
            "Number of columns: 31\n",
            "\n",
            "Schema:\n",
            "root\n",
            " |-- appid: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- release_date: date (nullable = true)\n",
            " |-- required_age: integer (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- dlc_count: integer (nullable = true)\n",
            " |-- windows: integer (nullable = true)\n",
            " |-- mac: integer (nullable = true)\n",
            " |-- linux: integer (nullable = true)\n",
            " |-- achievements: integer (nullable = true)\n",
            " |-- recommendations: integer (nullable = true)\n",
            " |-- supported_languages: string (nullable = true)\n",
            " |-- full_audio_languages: string (nullable = true)\n",
            " |-- developers: string (nullable = true)\n",
            " |-- publishers: string (nullable = true)\n",
            " |-- categories: string (nullable = true)\n",
            " |-- genres: string (nullable = true)\n",
            " |-- positive: integer (nullable = true)\n",
            " |-- negative: integer (nullable = true)\n",
            " |-- estimated_owners: string (nullable = true)\n",
            " |-- average_playtime_forever: integer (nullable = true)\n",
            " |-- average_playtime_2weeks: integer (nullable = true)\n",
            " |-- median_playtime_forever: integer (nullable = true)\n",
            " |-- median_playtime_2weeks: integer (nullable = true)\n",
            " |-- discount: integer (nullable = true)\n",
            " |-- peak_ccu: integer (nullable = true)\n",
            " |-- tags: string (nullable = true)\n",
            " |-- pct_pos_total: integer (nullable = true)\n",
            " |-- num_reviews_total: integer (nullable = true)\n",
            " |-- pct_pos_recent: integer (nullable = true)\n",
            " |-- num_reviews_recent: integer (nullable = true)\n",
            "\n",
            "\n",
            "Sample data:\n",
            "+------+-------------------------------+------------+------------+-----+---------+-------+---+-----+------------+---------------+--------------------------------------------------+--------------------------------------------------+--------------------+------------------+--------------------------------------------------+--------------------------------------------------+--------+--------+---------------------+------------------------+-----------------------+-----------------------+----------------------+--------+--------+--------------------------------------------------+-------------+-----------------+--------------+------------------+\n",
            "| appid|                           name|release_date|required_age|price|dlc_count|windows|mac|linux|achievements|recommendations|                               supported_languages|                              full_audio_languages|          developers|        publishers|                                        categories|                                            genres|positive|negative|     estimated_owners|average_playtime_forever|average_playtime_2weeks|median_playtime_forever|median_playtime_2weeks|discount|peak_ccu|                                              tags|pct_pos_total|num_reviews_total|pct_pos_recent|num_reviews_recent|\n",
            "+------+-------------------------------+------------+------------+-----+---------+-------+---+-----+------------+---------------+--------------------------------------------------+--------------------------------------------------+--------------------+------------------+--------------------------------------------------+--------------------------------------------------+--------+--------+---------------------+------------------------+-----------------------+-----------------------+----------------------+--------+--------+--------------------------------------------------+-------------+-----------------+--------------+------------------+\n",
            "|   730|               Counter-Strike 2|  2012-08-21|           0|  0.0|        1|      1|  0|    1|           1|        4401572|['Czech', 'Danish', 'Dutch', 'English', 'Finnis...|                         ['English', 'Indonesian']|           ['Valve']|         ['Valve']|['Multi-player', 'Cross-Platform Multiplayer', ...|                        ['Action', 'Free To Play']| 7480813| 1135108|100000000 - 200000000|                   33189|                    879|                   5174|                   350|       0| 1212356|{'FPS': 90857, 'Shooter': 65397, 'Multiplayer':...|           86|          8632939|            82|             96473|\n",
            "|578080|            PUBG: BATTLEGROUNDS|  2017-12-21|           0|  0.0|        0|      1|  0|    0|          37|        1732007|['English', 'Korean', 'Simplified Chinese', 'Fr...|                                                []|['PUBG Corporation']| ['KRAFTON, Inc.']|['Multi-player', 'PvP', 'Online PvP', 'Stats', ...|['Action', 'Adventure', 'Massively Multiplayer'...| 1487960| 1024436| 50000000 - 100000000|                       0|                      0|                      0|                     0|       0|  616738|{'Survival': 14838, 'Shooter': 12727, 'Battle R...|           59|          2513842|            68|             16720|\n",
            "|   570|                         Dota 2|  2013-07-09|           0|  0.0|        2|      1|  1|    1|           0|          14337|['Bulgarian', 'Czech', 'Danish', 'Dutch', 'Engl...|['English', 'Korean', 'Simplified Chinese', 'Vi...|           ['Valve']|         ['Valve']|['Multi-player', 'Co-op', 'Steam Trading Cards'...|            ['Action', 'Strategy', 'Free To Play']| 1998462|  451338|200000000 - 500000000|                   43031|                   1536|                    898|                   892|       0|  555977|{'Free to Play': 59933, 'MOBA': 20158, 'Multipl...|           81|          2452595|            80|             29366|\n",
            "|271590|      Grand Theft Auto V Legacy|  2015-04-13|          17|  0.0|        0|      1|  0|    0|          77|        1803063|['English', 'French', 'Italian', 'German', 'Spa...|            ['English', 'Spanish - Latin America']|  ['Rockstar North']|['Rockstar Games']|['Single-player', 'Multi-player', 'PvP', 'Onlin...|                           ['Action', 'Adventure']| 1719950|  250012| 50000000 - 100000000|                   19323|                    771|                   7101|                    74|       0|  117698|{'Open World': 32644, 'Action': 23539, 'Multipl...|           87|          1803832|            92|             17517|\n",
            "|359550|Tom Clancy's Rainbow SixÂ® Siege|  2015-12-01|          17| 3.99|        9|      1|  0|    0|           0|        1165929|['English', 'French', 'Italian', 'German', 'Spa...|['English', 'French', 'Italian', 'German', 'Spa...|['Ubisoft Montreal']|       ['Ubisoft']|['Single-player', 'Multi-player', 'PvP', 'Onlin...|                                        ['Action']| 1152763|  218446|  20000000 - 50000000|                   14204|                    682|                   2434|                   306|      80|   89916|{'FPS': 9831, 'PvP': 9162, 'e-sports': 9072, 'M...|           84|          1168020|            76|             12608|\n",
            "+------+-------------------------------+------------+------------+-----+---------+-------+---+-----+------------+---------------+--------------------------------------------------+--------------------------------------------------+--------------------+------------------+--------------------------------------------------+--------------------------------------------------+--------+--------+---------------------+------------------------+-----------------------+-----------------------+----------------------+--------+--------+--------------------------------------------------+-------------+-----------------+--------------+------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "# Read preprocessed CSV file\n",
        "df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .option(\"escape\", '\"') \\\n",
        "    .option(\"multiLine\", \"true\") \\\n",
        "    .option(\"quote\", '\"') \\\n",
        "    .csv(\"../archive1/games_march2025_ml_ready.csv\")\n",
        "\n",
        "print(f\"Dataset loaded:\")\n",
        "print(f\"Total number of games: {df.count():,}\")\n",
        "print(f\"Number of columns: {len(df.columns)}\")\n",
        "print(\"\\nSchema:\")\n",
        "df.printSchema()\n",
        "\n",
        "# Show sample\n",
        "print(\"\\nSample data:\")\n",
        "df.show(5, truncate=50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Handle Null Values\n",
        "\n",
        "Fill or remove null values in the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "NULL VALUES BEFORE HANDLING\n",
            "================================================================================\n",
            "No null values found!\n"
          ]
        }
      ],
      "source": [
        "# Check null values before handling\n",
        "print(\"=\"*80)\n",
        "print(\"NULL VALUES BEFORE HANDLING\")\n",
        "print(\"=\"*80)\n",
        "total_rows = df.count()\n",
        "null_summary = []\n",
        "for col_name in df.columns:\n",
        "    null_count = df.filter(col(col_name).isNull()).count()\n",
        "    null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0\n",
        "    if null_count > 0:\n",
        "        null_summary.append((col_name, null_count, null_pct))\n",
        "\n",
        "if null_summary:\n",
        "    print(f\"{'Column':<30} {'Null Count':<15} {'Null %':<15}\")\n",
        "    print(\"-\"*80)\n",
        "    for col_name, null_count, null_pct in sorted(null_summary, key=lambda x: x[1], reverse=True):\n",
        "        print(f\"{col_name:<30} {null_count:<15,} {null_pct:<15.2f}%\")\n",
        "else:\n",
        "    print(\"No null values found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null values handled!\n"
          ]
        }
      ],
      "source": [
        "# Handle null values\n",
        "# Strategy:\n",
        "# - Numerical columns: Fill with 0 or median\n",
        "# - Categorical columns: Fill with \"Unknown\" or empty string\n",
        "# - Date columns: Keep as null or fill with a default date\n",
        "\n",
        "df_cleaned = df\n",
        "\n",
        "# Fill numerical nulls with 0\n",
        "numerical_cols_to_fill = [\n",
        "    'dlc_count', 'required_age', 'achievements', 'recommendations',\n",
        "    'positive', 'negative', 'average_playtime_forever', 'average_playtime_2weeks',\n",
        "    'median_playtime_forever', 'median_playtime_2weeks', 'discount', 'peak_ccu',\n",
        "    'pct_pos_total', 'num_reviews_total', 'pct_pos_recent', 'num_reviews_recent'\n",
        "]\n",
        "\n",
        "for col_name in numerical_cols_to_fill:\n",
        "    if col_name in df_cleaned.columns:\n",
        "        df_cleaned = df_cleaned.withColumn(\n",
        "            col_name,\n",
        "            when(col(col_name).isNull(), 0).otherwise(col(col_name))\n",
        "        )\n",
        "\n",
        "# Fill price nulls with 0 (free games)\n",
        "if 'price' in df_cleaned.columns:\n",
        "    df_cleaned = df_cleaned.withColumn(\n",
        "        'price',\n",
        "        when(col('price').isNull(), 0.0).otherwise(col('price'))\n",
        "    )\n",
        "\n",
        "# Fill categorical nulls with \"Unknown\"\n",
        "categorical_cols_to_fill = [\n",
        "    'developers', 'publishers', 'categories', 'genres',\n",
        "    'supported_languages', 'full_audio_languages', 'tags', 'estimated_owners'\n",
        "]\n",
        "\n",
        "for col_name in categorical_cols_to_fill:\n",
        "    if col_name in df_cleaned.columns:\n",
        "        df_cleaned = df_cleaned.withColumn(\n",
        "            col_name,\n",
        "            when(col(col_name).isNull(), \"Unknown\").otherwise(col(col_name))\n",
        "        )\n",
        "\n",
        "# Fill name nulls\n",
        "if 'name' in df_cleaned.columns:\n",
        "    df_cleaned = df_cleaned.withColumn(\n",
        "        'name',\n",
        "        when(col('name').isNull(), \"Unknown Game\").otherwise(col('name'))\n",
        "    )\n",
        "\n",
        "print(\"Null values handled!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "NULL VALUES AFTER HANDLING\n",
            "================================================================================\n",
            "All null values have been handled!\n"
          ]
        }
      ],
      "source": [
        "# Verify null values after handling\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"NULL VALUES AFTER HANDLING\")\n",
        "print(\"=\"*80)\n",
        "total_rows = df_cleaned.count()\n",
        "null_summary_after = []\n",
        "for col_name in df_cleaned.columns:\n",
        "    null_count = df_cleaned.filter(col(col_name).isNull()).count()\n",
        "    if null_count > 0:\n",
        "        null_summary_after.append((col_name, null_count))\n",
        "\n",
        "if null_summary_after:\n",
        "    print(f\"{'Column':<30} {'Null Count':<15}\")\n",
        "    print(\"-\"*80)\n",
        "    for col_name, null_count in null_summary_after:\n",
        "        print(f\"{col_name:<30} {null_count:<15,}\")\n",
        "else:\n",
        "    print(\"All null values have been handled!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Encode Categorical Variables\n",
        "\n",
        "Convert categorical variables (developers, publishers, genres, categories) to numerical format using StringIndexer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Categorical columns extracted:\n",
            "  - developers -> developers_main\n",
            "+--------------------+----------------+\n",
            "|developers          |developers_main |\n",
            "+--------------------+----------------+\n",
            "|['Valve']           |Valve           |\n",
            "|['PUBG Corporation']|PUBG Corporation|\n",
            "|['Valve']           |Valve           |\n",
            "|['Rockstar North']  |Rockstar North  |\n",
            "|['Ubisoft Montreal']|Ubisoft Montreal|\n",
            "+--------------------+----------------+\n",
            "only showing top 5 rows\n",
            "  - publishers -> publishers_main\n",
            "+------------------+---------------+\n",
            "|publishers        |publishers_main|\n",
            "+------------------+---------------+\n",
            "|['Valve']         |Valve          |\n",
            "|['KRAFTON, Inc.'] |KRAFTON        |\n",
            "|['Valve']         |Valve          |\n",
            "|['Rockstar Games']|Rockstar Games |\n",
            "|['Ubisoft']       |Ubisoft        |\n",
            "+------------------+---------------+\n",
            "only showing top 5 rows\n",
            "  - genres -> genres_main\n",
            "+----------------------------------------------------------------+-----------+\n",
            "|genres                                                          |genres_main|\n",
            "+----------------------------------------------------------------+-----------+\n",
            "|['Action', 'Free To Play']                                      |Action     |\n",
            "|['Action', 'Adventure', 'Massively Multiplayer', 'Free To Play']|Action     |\n",
            "|['Action', 'Strategy', 'Free To Play']                          |Action     |\n",
            "|['Action', 'Adventure']                                         |Action     |\n",
            "|['Action']                                                      |Action     |\n",
            "+----------------------------------------------------------------+-----------+\n",
            "only showing top 5 rows\n",
            "  - categories -> categories_main\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
            "|categories                                                                                                                                                                                                                              |categories_main|\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
            "|['Multi-player', 'Cross-Platform Multiplayer', 'Steam Trading Cards', 'Steam Workshop', 'In-App Purchases', 'Valve Anti-Cheat enabled', 'Stats', 'Remote Play on Phone', 'Remote Play on Tablet', 'Remote Play on TV', 'Steam Timeline']|Multi-player   |\n",
            "|['Multi-player', 'PvP', 'Online PvP', 'Stats', 'Remote Play on Phone', 'Remote Play on Tablet']                                                                                                                                         |Multi-player   |\n",
            "|['Multi-player', 'Co-op', 'Steam Trading Cards', 'Steam Workshop', 'SteamVR Collectibles', 'In-App Purchases', 'Valve Anti-Cheat enabled', 'Steam Timeline']                                                                            |Multi-player   |\n",
            "|['Single-player', 'Multi-player', 'PvP', 'Online PvP', 'Co-op', 'Online Co-op', 'Steam Achievements', 'Full controller support', 'Remote Play on Phone', 'Remote Play on Tablet', 'Remote Play on TV']                                  |Single-player  |\n",
            "|['Single-player', 'Multi-player', 'PvP', 'Online PvP', 'Co-op', 'Online Co-op', 'Full controller support', 'Steam Trading Cards', 'In-App Purchases', 'Remote Play on Phone', 'Remote Play on Tablet']                                  |Single-player  |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "# Parse list-like strings and extract first/main value for encoding\n",
        "# For example: \"['Valve']\" -> \"Valve\", \"['Publisher1', 'Publisher2']\" -> \"Publisher1\"\n",
        "\n",
        "def extract_first_value(value_str):\n",
        "    \"\"\"Extract first value from string representation of list\"\"\"\n",
        "    if value_str is None or value_str == \"Unknown\" or value_str == \"\":\n",
        "        return \"Unknown\"\n",
        "    try:\n",
        "        # Remove brackets and quotes, get first value\n",
        "        value_str = str(value_str).strip()\n",
        "        if value_str.startswith('[') and value_str.endswith(']'):\n",
        "            value_str = value_str[1:-1]\n",
        "        # Split by comma and get first\n",
        "        values = [v.strip().strip(\"'\\\"\") for v in value_str.split(',') if v.strip()]\n",
        "        return values[0] if values else \"Unknown\"\n",
        "    except:\n",
        "        return \"Unknown\"\n",
        "\n",
        "# Apply extraction to categorical columns\n",
        "from pyspark.sql.udf import UserDefinedFunction\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "extract_first_udf = UserDefinedFunction(extract_first_value, StringType())\n",
        "\n",
        "categorical_cols = ['developers', 'publishers', 'genres', 'categories']\n",
        "df_encoded = df_cleaned\n",
        "\n",
        "for col_name in categorical_cols:\n",
        "    if col_name in df_encoded.columns:\n",
        "        df_encoded = df_encoded.withColumn(\n",
        "            f\"{col_name}_main\",\n",
        "            extract_first_udf(col(col_name))\n",
        "        )\n",
        "\n",
        "print(\"Categorical columns extracted:\")\n",
        "for col_name in categorical_cols:\n",
        "    if f\"{col_name}_main\" in df_encoded.columns:\n",
        "        print(f\"  - {col_name} -> {col_name}_main\")\n",
        "        df_encoded.select(col_name, f\"{col_name}_main\").show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting StringIndexers...\n",
            "\n",
            "Encoded 4 categorical columns:\n",
            "  - developers_main_indexed\n",
            "  - publishers_main_indexed\n",
            "  - genres_main_indexed\n",
            "  - categories_main_indexed\n",
            "\n",
            "Sample encoded values:\n",
            "+-----------------+-----------------------+-----------------+-----------------------+\n",
            "|developers_main  |developers_main_indexed|publishers_main  |publishers_main_indexed|\n",
            "+-----------------+-----------------------+-----------------+-----------------------+\n",
            "|Valve            |24.0                   |Valve            |81.0                   |\n",
            "|PUBG Corporation |5246.0                 |KRAFTON          |337.0                  |\n",
            "|Valve            |24.0                   |Valve            |81.0                   |\n",
            "|Rockstar North   |700.0                  |Rockstar Games   |115.0                  |\n",
            "|Ubisoft Montreal |19.0                   |Ubisoft          |5.0                    |\n",
            "|Valve            |24.0                   |Valve            |81.0                   |\n",
            "|Re-Logic         |5596.0                 |Re-Logic         |1292.0                 |\n",
            "|Facepunch Studios|341.0                  |Facepunch Studios|583.0                  |\n",
            "|Facepunch Studios|341.0                  |Valve            |81.0                   |\n",
            "|Respawn          |1588.0                 |Electronic Arts  |4.0                    |\n",
            "+-----------------+-----------------------+-----------------+-----------------------+\n",
            "only showing top 10 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/home/ismail/pyspark_env/lib64/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 233, in manager\n",
            "    code = worker(sock, authenticated)\n",
            "  File \"/home/ismail/pyspark_env/lib64/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 87, in worker\n",
            "    outfile.flush()\n",
            "    ~~~~~~~~~~~~~^^\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ]
        }
      ],
      "source": [
        "# Use StringIndexer to encode categorical variables\n",
        "# StringIndexer converts string categories to numerical indices\n",
        "\n",
        "indexers = []\n",
        "indexed_cols = []\n",
        "\n",
        "categorical_cols_to_index = ['developers_main', 'publishers_main', 'genres_main', 'categories_main']\n",
        "\n",
        "for col_name in categorical_cols_to_index:\n",
        "    if col_name in df_encoded.columns:\n",
        "        indexer = StringIndexer(\n",
        "            inputCol=col_name,\n",
        "            outputCol=f\"{col_name}_indexed\",\n",
        "            handleInvalid=\"keep\"  # Keep unknown values as a separate category\n",
        "        )\n",
        "        indexers.append(indexer)\n",
        "        indexed_cols.append(f\"{col_name}_indexed\")\n",
        "\n",
        "# Fit and transform\n",
        "print(\"Fitting StringIndexers...\")\n",
        "for indexer in indexers:\n",
        "    df_encoded = indexer.fit(df_encoded).transform(df_encoded)\n",
        "\n",
        "print(f\"\\nEncoded {len(indexed_cols)} categorical columns:\")\n",
        "for col_name in indexed_cols:\n",
        "    print(f\"  - {col_name}\")\n",
        "\n",
        "# Show sample of encoded values\n",
        "print(\"\\nSample encoded values:\")\n",
        "sample_cols = ['developers_main', 'developers_main_indexed', 'publishers_main', 'publishers_main_indexed']\n",
        "existing_sample_cols = [c for c in sample_cols if c in df_encoded.columns]\n",
        "if existing_sample_cols:\n",
        "    df_encoded.select(existing_sample_cols).show(10, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Prepare Numerical Features\n",
        "\n",
        "Select and prepare numerical features for ML models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected 22 features for ML:\n",
            " 1. price\n",
            " 2. dlc_count\n",
            " 3. required_age\n",
            " 4. achievements\n",
            " 5. recommendations\n",
            " 6. positive\n",
            " 7. negative\n",
            " 8. average_playtime_forever\n",
            " 9. median_playtime_forever\n",
            "10. discount\n",
            "11. peak_ccu\n",
            "12. pct_pos_total\n",
            "13. num_reviews_total\n",
            "14. pct_pos_recent\n",
            "15. num_reviews_recent\n",
            "16. windows\n",
            "17. mac\n",
            "18. linux\n",
            "19. developers_main_indexed\n",
            "20. publishers_main_indexed\n",
            "21. genres_main_indexed\n",
            "22. categories_main_indexed\n",
            "\n",
            "Checking for nulls in feature columns:\n",
            "  price: No nulls\n",
            "  dlc_count: No nulls\n",
            "  required_age: No nulls\n",
            "  achievements: No nulls\n",
            "  recommendations: No nulls\n",
            "  positive: No nulls\n",
            "  negative: No nulls\n",
            "  average_playtime_forever: No nulls\n",
            "  median_playtime_forever: No nulls\n",
            "  discount: No nulls\n",
            "  peak_ccu: No nulls\n",
            "  pct_pos_total: No nulls\n",
            "  num_reviews_total: No nulls\n",
            "  pct_pos_recent: No nulls\n",
            "  num_reviews_recent: No nulls\n",
            "  windows: No nulls\n",
            "  mac: No nulls\n",
            "  linux: No nulls\n",
            "  developers_main_indexed: No nulls\n",
            "  publishers_main_indexed: No nulls\n",
            "  genres_main_indexed: No nulls\n",
            "  categories_main_indexed: No nulls\n"
          ]
        }
      ],
      "source": [
        "# Select numerical features for ML\n",
        "numerical_features = [\n",
        "    'price', 'dlc_count', 'required_age',\n",
        "    'achievements', 'recommendations', 'positive', 'negative',\n",
        "    'average_playtime_forever', 'median_playtime_forever',\n",
        "    'discount', 'peak_ccu', 'pct_pos_total', 'num_reviews_total',\n",
        "    'pct_pos_recent', 'num_reviews_recent',\n",
        "    'windows', 'mac', 'linux'\n",
        "]\n",
        "\n",
        "# Add indexed categorical features\n",
        "all_features = numerical_features + indexed_cols\n",
        "\n",
        "# Filter to only existing columns\n",
        "existing_features = [col for col in all_features if col in df_encoded.columns]\n",
        "\n",
        "print(f\"Selected {len(existing_features)} features for ML:\")\n",
        "for i, feat in enumerate(existing_features, 1):\n",
        "    print(f\"{i:2d}. {feat}\")\n",
        "\n",
        "# Check for any nulls in feature columns\n",
        "print(\"\\nChecking for nulls in feature columns:\")\n",
        "for feat in existing_features:\n",
        "    null_count = df_encoded.filter(col(feat).isNull()).count()\n",
        "    if null_count > 0:\n",
        "        print(f\"  {feat}: {null_count} nulls\")\n",
        "    else:\n",
        "        print(f\"  {feat}: No nulls\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Scale/Normalize Numerical Features\n",
        "\n",
        "Apply StandardScaler or MinMaxScaler to normalize numerical features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numerical features to scale: 18\n",
            "Categorical features (already indexed): 4\n",
            "\n",
            "Numerical features scaled using StandardScaler\n",
            "\n",
            "Sample of scaled features:\n",
            "+--------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|numerical_features                                                                                                  |scaled_numerical_features                                                                                                                                                                                                                                                                                                                         |\n",
            "+--------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[0.0,1.0,0.0,1.0,4401572.0,7480813.0,1135108.0,33189.0,5174.0,0.0,1212356.0,86.0,8632939.0,82.0,96473.0,1.0,0.0,1.0]|[-0.9617916463222029,-0.04411777919573288,-0.23926883230488932,-0.21598023213278722,73.63086699022556,86.31056765245181,67.7655397639962,2.504704003763754,0.22548563757096127,-0.35900237395956597,77.49053291782351,0.31851307595604794,89.04427654112496,0.983883961445711,79.59973822169094,0.0,-0.6705694825732949,1.9517032561423326]       |\n",
            "|(18,[3,4,5,6,10,11,12,13,14,15],[37.0,1732007.0,1487960.0,1024436.0,616738.0,59.0,2513842.0,68.0,16720.0,1.0])      |[-0.9617916463222029,-0.06915913682287254,-0.23926883230488932,-0.0322415452628481,28.89814121923195,17.081770964732954,61.14967495919515,-0.0505541714513617,-0.0323096115654233,-0.35900237395956597,39.39780523056667,-1.7471821907548968,25.857958098685693,0.6583174234439725,13.713396611142555,0.0,-0.6705694825732949,-0.5123298767318516]|\n",
            "|[0.0,2.0,0.0,0.0,14337.0,1998462.0,451338.0,43031.0,898.0,0.0,555977.0,81.0,2452595.0,80.0,29366.0,1.0,1.0,1.0]     |[-0.9617916463222029,-0.019076421568593214,-0.23926883230488932,-0.22108408454584108,0.11590101340717834,22.979035450725494,26.89043244164898,3.262450575232228,0.012433359776763277,-0.35900237395956597,35.51183766913398,-0.06402308454597884,25.22551634369405,0.9373744560168912,24.16063591257316,0.0,1.4911443401799327,1.9517032561423326]|\n",
            "+--------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 3 rows\n"
          ]
        }
      ],
      "source": [
        "# Separate numerical and categorical features\n",
        "numerical_features_clean = [f for f in numerical_features if f in df_encoded.columns]\n",
        "categorical_features_clean = [f for f in indexed_cols if f in df_encoded.columns]\n",
        "\n",
        "print(f\"Numerical features to scale: {len(numerical_features_clean)}\")\n",
        "print(f\"Categorical features (already indexed): {len(categorical_features_clean)}\")\n",
        "\n",
        "# Create feature vector using VectorAssembler\n",
        "# First assemble numerical features\n",
        "assembler_numerical = VectorAssembler(\n",
        "    inputCols=numerical_features_clean,\n",
        "    outputCol=\"numerical_features\",\n",
        "    handleInvalid=\"skip\"\n",
        ")\n",
        "\n",
        "df_features = assembler_numerical.transform(df_encoded)\n",
        "\n",
        "# Scale numerical features using StandardScaler\n",
        "scaler = StandardScaler(\n",
        "    inputCol=\"numerical_features\",\n",
        "    outputCol=\"scaled_numerical_features\",\n",
        "    withStd=True,\n",
        "    withMean=True\n",
        ")\n",
        "\n",
        "scaler_model = scaler.fit(df_features)\n",
        "df_features = scaler_model.transform(df_features)\n",
        "\n",
        "print(\"\\nNumerical features scaled using StandardScaler\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\nSample of scaled features:\")\n",
        "df_features.select(\"numerical_features\", \"scaled_numerical_features\").show(3, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "SCALING COMPARISON: Original vs Scaled Features\n",
            "================================================================================\n",
            "\n",
            "SIDE-BY-SIDE COMPARISON (First 3 Features):\n",
            "================================================================================\n",
            "\n",
            "price:\n",
            "------------------------------------------------------------\n",
            "  Original values (first 5):\n",
            "    0.0\n",
            "    0.0\n",
            "    0.0\n",
            "    0.0\n",
            "    3.99\n",
            "  Scaled values (first 5):\n",
            "    -0.9618\n",
            "    -0.9618\n",
            "    -0.9618\n",
            "    -0.9618\n",
            "    -0.6555\n",
            "\n",
            "dlc_count:\n",
            "------------------------------------------------------------\n",
            "  Original values (first 5):\n",
            "    1\n",
            "    0\n",
            "    2\n",
            "    0\n",
            "    9\n",
            "  Scaled values (first 5):\n",
            "    -0.0441\n",
            "    -0.0692\n",
            "    -0.0191\n",
            "    -0.0692\n",
            "    0.1562\n",
            "\n",
            "required_age:\n",
            "------------------------------------------------------------\n",
            "  Original values (first 5):\n",
            "    0\n",
            "    0\n",
            "    0\n",
            "    17\n",
            "    17\n",
            "  Scaled values (first 5):\n",
            "    -0.2393\n",
            "    -0.2393\n",
            "    -0.2393\n",
            "    4.2973\n",
            "    4.2973\n",
            "\n",
            "STATISTICS COMPARISON:\n",
            "================================================================================\n",
            "\n",
            "BEFORE SCALING - Sample features statistics:\n",
            "+-------+------------------+------------------+------------------+\n",
            "|summary|             price|         dlc_count|      required_age|\n",
            "+-------+------------------+------------------+------------------+\n",
            "|  count|             11889|             11889|             11889|\n",
            "|   mean|12.528331230549863|2.7617966187231895|0.8966271343258474|\n",
            "| stddev|13.026034566276827| 39.93393708479312| 3.747362854110959|\n",
            "|    min|               0.0|                 0|                -1|\n",
            "|    max|             89.99|              3427|                18|\n",
            "+-------+------------------+------------------+------------------+\n",
            "\n",
            "\n",
            "AFTER SCALING - Mean and Std should be ~0 and ~1:\n",
            "  First feature after scaling:\n",
            "    Mean: -0.000000 (should be ~0)\n",
            "    Std:  1.000000 (should be ~1)\n",
            "\n",
            "Scaling verification complete!\n"
          ]
        }
      ],
      "source": [
        "# Quick comparison: Original vs Scaled Features\n",
        "print(\"=\"*80)\n",
        "print(\"SCALING COMPARISON: Original vs Scaled Features\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Show a few example features before and after scaling\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "from pyspark.sql.functions import col as spark_col\n",
        "\n",
        "# Extract scaled values from vector\n",
        "df_scaled_check = df_features.withColumn(\n",
        "    \"scaled_array\", \n",
        "    vector_to_array(\"scaled_numerical_features\")\n",
        ")\n",
        "\n",
        "# Show comparison for first 3 numerical features\n",
        "print(\"\\nSIDE-BY-SIDE COMPARISON (First 3 Features):\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i in range(min(3, len(numerical_features_clean))):\n",
        "    feature_name = numerical_features_clean[i]\n",
        "    print(f\"\\n{feature_name}:\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Original values\n",
        "    original_vals = df_encoded.select(feature_name).limit(5).collect()\n",
        "    print(\"  Original values (first 5):\")\n",
        "    for row in original_vals:\n",
        "        val = row[feature_name]\n",
        "        print(f\"    {val}\")\n",
        "    \n",
        "    # Scaled values\n",
        "    scaled_col = spark_col(\"scaled_array\")[i].alias(f\"{feature_name}_scaled\")\n",
        "    scaled_vals = df_scaled_check.select(scaled_col).limit(5).collect()\n",
        "    print(\"  Scaled values (first 5):\")\n",
        "    for row in scaled_vals:\n",
        "        val = row[f\"{feature_name}_scaled\"]\n",
        "        print(f\"    {val:.4f}\")\n",
        "\n",
        "# Show statistics\n",
        "print(\"\\nSTATISTICS COMPARISON:\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nBEFORE SCALING - Sample features statistics:\")\n",
        "df_encoded.select(numerical_features_clean[:3]).describe().show()\n",
        "\n",
        "print(\"\\nAFTER SCALING - Mean and Std should be ~0 and ~1:\")\n",
        "# Check mean and std of first scaled feature\n",
        "from pyspark.sql.functions import mean as spark_mean, stddev as spark_stddev\n",
        "scaled_stats = df_scaled_check.select(\n",
        "    spark_mean(spark_col(\"scaled_array\")[0]).alias(\"mean\"),\n",
        "    spark_stddev(spark_col(\"scaled_array\")[0]).alias(\"std\")\n",
        ").collect()\n",
        "\n",
        "if scaled_stats:\n",
        "    print(f\"  First feature after scaling:\")\n",
        "    print(f\"    Mean: {scaled_stats[0].mean:.6f} (should be ~0)\")\n",
        "    print(f\"    Std:  {scaled_stats[0].std:.6f} (should be ~1)\")\n",
        "\n",
        "print(\"\\nScaling verification complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final feature vector created!\n",
            "Feature vector size: 18 scaled numerical + 4 categorical\n",
            "\n",
            "Sample of final features:\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|features                                                                                                                                                                                                                                                                                                                                                               |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[-0.9617916463222029,-0.04411777919573288,-0.23926883230488932,-0.21598023213278722,73.63086699022556,86.31056765245181,67.7655397639962,2.504704003763754,0.22548563757096127,-0.35900237395956597,77.49053291782351,0.31851307595604794,89.04427654112496,0.983883961445711,79.59973822169094,0.0,-0.6705694825732949,1.9517032561423326,24.0,81.0,0.0,1.0]          |\n",
            "|[-0.9617916463222029,-0.06915913682287254,-0.23926883230488932,-0.0322415452628481,28.89814121923195,17.081770964732954,61.14967495919515,-0.0505541714513617,-0.0323096115654233,-0.35900237395956597,39.39780523056667,-1.7471821907548968,25.857958098685693,0.6583174234439725,13.713396611142555,0.0,-0.6705694825732949,-0.5123298767318516,5246.0,337.0,0.0,1.0]|\n",
            "|[-0.9617916463222029,-0.019076421568593214,-0.23926883230488932,-0.22108408454584108,0.11590101340717834,22.979035450725494,26.89043244164898,3.262450575232228,0.012433359776763277,-0.35900237395956597,35.51183766913398,-0.06402308454597884,25.22551634369405,0.9373744560168912,24.16063591257316,0.0,1.4911443401799327,1.9517032561423326,24.0,81.0,0.0,1.0]   |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 3 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/home/ismail/pyspark_env/lib64/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 233, in manager\n",
            "    code = worker(sock, authenticated)\n",
            "  File \"/home/ismail/pyspark_env/lib64/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 87, in worker\n",
            "    outfile.flush()\n",
            "    ~~~~~~~~~~~~~^^\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ]
        }
      ],
      "source": [
        "# Combine scaled numerical features with categorical features\n",
        "# Create final feature vector\n",
        "final_feature_cols = [\"scaled_numerical_features\"] + categorical_features_clean\n",
        "\n",
        "assembler_final = VectorAssembler(\n",
        "    inputCols=final_feature_cols,\n",
        "    outputCol=\"features\",\n",
        "    handleInvalid=\"skip\"\n",
        ")\n",
        "\n",
        "df_ml_ready = assembler_final.transform(df_features)\n",
        "\n",
        "print(\"Final feature vector created!\")\n",
        "print(f\"Feature vector size: {len(numerical_features_clean)} scaled numerical + {len(categorical_features_clean)} categorical\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\nSample of final features:\")\n",
        "df_ml_ready.select(\"features\").show(3, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Split into Train/Test Sets\n",
        "\n",
        "Split the dataset into training and testing sets (typically 80/20 or 70/30).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "DATASET SPLIT\n",
            "================================================================================\n",
            "Training set: 9,576 samples (80.5%)\n",
            "Test set: 2,313 samples (19.5%)\n",
            "Total: 11,889 samples\n",
            "\n",
            " Datasets cached for faster ML operations\n"
          ]
        }
      ],
      "source": [
        "# Split into train and test sets\n",
        "# 80% training, 20% testing\n",
        "train_df, test_df = df_ml_ready.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DATASET SPLIT\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Training set: {train_df.count():,} samples ({train_df.count()/df_ml_ready.count()*100:.1f}%)\")\n",
        "print(f\"Test set: {test_df.count():,} samples ({test_df.count()/df_ml_ready.count()*100:.1f}%)\")\n",
        "print(f\"Total: {df_ml_ready.count():,} samples\")\n",
        "\n",
        "# Cache the datasets for faster access during ML training\n",
        "train_df.cache()\n",
        "test_df.cache()\n",
        "\n",
        "print(\"\\n Datasets cached for faster ML operations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Save ML-Ready Data\n",
        "\n",
        "Save the prepared datasets for ML model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving training set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/12/25 18:12:23 WARN DAGScheduler: Broadcasting large task binary with size 1395.8 KiB\n",
            "25/12/25 18:12:24 WARN DAGScheduler: Broadcasting large task binary with size 1395.8 KiB\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set saved: archive1/train_ml_ready.parquet\n",
            "Saving test set...\n",
            "Test set saved: archive1/test_ml_ready.parquet\n",
            "\n",
            "================================================================================\n",
            "ML DATA PREPARATION COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "Training set: archive1/train_ml_ready.parquet\n",
            "Test set: archive1/test_ml_ready.parquet\n",
            "\n",
            "Features prepared: 22\n",
            "  - Numerical (scaled): 18\n",
            "  - Categorical (indexed): 4\n",
            "\n",
            "Ready for ML model training!\n"
          ]
        }
      ],
      "source": [
        "# Select key columns for saving (including features)\n",
        "columns_to_save = ['appid', 'name'] + existing_features + ['features']\n",
        "\n",
        "# Filter to existing columns\n",
        "columns_to_save = [c for c in columns_to_save if c in df_ml_ready.columns]\n",
        "\n",
        "# Save training set\n",
        "print(\"Saving training set...\")\n",
        "train_output = \"../archive1/train_ml_ready.parquet\"\n",
        "train_df.select(columns_to_save).write.mode(\"overwrite\").parquet(train_output)\n",
        "print(f\"Training set saved: {train_output}\")\n",
        "\n",
        "# Save test set\n",
        "print(\"Saving test set...\")\n",
        "test_output = \"../archive1/test_ml_ready.parquet\"\n",
        "test_df.select(columns_to_save).write.mode(\"overwrite\").parquet(test_output)\n",
        "print(f\"Test set saved: {test_output}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ML DATA PREPARATION COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nTraining set: {train_output}\")\n",
        "print(f\"Test set: {test_output}\")\n",
        "print(f\"\\nFeatures prepared: {len(existing_features)}\")\n",
        "print(f\"  - Numerical (scaled): {len(numerical_features_clean)}\")\n",
        "print(f\"  - Categorical (indexed): {len(categorical_features_clean)}\")\n",
        "print(f\"\\nReady for ML model training!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### ML Preparation Steps Completed:\n",
        "\n",
        "1. **Handled null values:**\n",
        "   - Numerical columns: Filled with 0\n",
        "   - Categorical columns: Filled with \"Unknown\"\n",
        "   - Price: Filled with 0.0 (free games)\n",
        "\n",
        "2. **Encoded categorical variables:**\n",
        "   - Extracted main value from list-like strings (developers, publishers, genres, categories)\n",
        "   - Used StringIndexer to convert to numerical indices\n",
        "   - Handled unknown values\n",
        "\n",
        "3. **Scaled numerical features:**\n",
        "   - Used StandardScaler (mean=0, std=1)\n",
        "   - Normalized all numerical features\n",
        "\n",
        "4. **Created feature vectors:**\n",
        "   - Combined scaled numerical features with indexed categorical features\n",
        "   - Created final \"features\" column ready for ML\n",
        "\n",
        "5. **Split into train/test:**\n",
        "   - 80% training set\n",
        "   - 20% test set\n",
        "   - Both cached for faster access\n",
        "\n",
        "6. **Saved ML-ready data:**\n",
        "   - Training set: `../archive1/train_ml_ready.parquet`\n",
        "   - Test set: `../archive1/test_ml_ready.parquet`\n",
        "\n",
        "### Next Steps:\n",
        "- Load the parquet files for ML model training\n",
        "- Use the \"features\" column as input for ML algorithms\n",
        "- Choose appropriate ML algorithms (classification, regression, clustering, etc.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session stopped.\n"
          ]
        }
      ],
      "source": [
        "# Clean up\n",
        "spark.stop()\n",
        "print(\"Spark session stopped.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pyspark_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
